{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Autoencoder Model for Real-time Anomaly Predictions in Distributed Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EK1n1axBw_HI",
    "outputId": "30c5205a-b0dd-4e92-e6ac-d1e75a2e5900"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import LSTM, Bidirectional, Dense, RepeatVector, TimeDistributed, Input\n",
    "from keras.utils import plot_model\n",
    "\n",
    "#others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "from slack_utils import send_to_slack\n",
    "from dataExtraction import get_data_x_minutes_back_as_dataframe\n",
    "from datetime import datetime\n",
    "\n",
    "#Uncomment these two lines when running the system on terminal server\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TvxTpHi4GRj"
   },
   "source": [
    "## Defining Simple Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlTnDWsP5zyG"
   },
   "outputs": [],
   "source": [
    "# Here both input and output sequence length should be same as we are just trying to reconstruct the sequence\n",
    "\n",
    "def get_model(timesteps, input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(RepeatVector(timesteps))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(input_dim)))\n",
    "    \n",
    "    # adam optimizer with lr=0.001 is used.\n",
    "    # For reconstruction error, mse is the metric we use\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPoB78sDsr-u"
   },
   "outputs": [],
   "source": [
    "# Here both input and output sequence length should be same as we are just trying to reconstruct the sequence\n",
    "# Sample model that was experimented with while hyperparameter tuning.\n",
    "# Uncomment if you wish to try out this model\n",
    "\n",
    "# def get_model(timesteps, input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(500, activation='relu', input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "#     model.add(Bidirectional(LSTM(256)))\n",
    "#     model.add(RepeatVector(timesteps))\n",
    "#     model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(input_dim)))\n",
    "    \n",
    "#     # adam optimizer with lr=0.001 is used.\n",
    "#     # For reconstruction error, mse is the metric we use\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='mse')\n",
    "#     print(model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4LcqoKW4--U"
   },
   "source": [
    "## Defining a Predictor + Decoder LSTM Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ3j2ykc5DqY"
   },
   "outputs": [],
   "source": [
    "# An advanced model that returns both previous states and current predictions\n",
    "# An lstm autoencoder model to reconstruct and predict sequence\n",
    "\n",
    "def model_advanced(look_back, look_ahead, n_features):\n",
    "    visible = Input(shape=(look_back, n_features))\n",
    "    encoder = LSTM(100, activation='relu')(visible)\n",
    "    # define reconstruct decoder\n",
    "    decoder1 = RepeatVector(look_back)(encoder)\n",
    "    decoder1 = LSTM(100, activation='relu', return_sequences=True)(decoder1)\n",
    "    decoder1 = TimeDistributed(Dense(n_features))(decoder1)\n",
    "    # define predict decoder\n",
    "    decoder2 = RepeatVector(look_ahead)(encoder)\n",
    "    decoder2 = LSTM(100, activation='relu', return_sequences=True)(decoder2)\n",
    "    decoder2 = TimeDistributed(Dense(n_features))(decoder2)\n",
    "    # combine it all together\n",
    "    model = Model(inputs=visible, outputs=[decoder1, decoder2])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1R9unBmWYP7C"
   },
   "source": [
    "## Defining Standard Scaling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6X4MAIdsYQPs"
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data, scaler\n",
    "\n",
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LevjJ41c8yrx"
   },
   "source": [
    "## Creating Input/Output Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t416l0Cc8yIh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We create subsequences with a defined window size and 50% overlap. \n",
    "These sequences are fed to the LSTM Autoencoder model and the reconstruction errors are assessed.\n",
    "Note: It is very important to reshape this sequence in the exact format as expected by our LSTM Autoencoder model.\n",
    "Every sequence must be reshaped to have a shape of [number_of_sample, time_steps, num_features]\n",
    "'''\n",
    "def create_subsequence(sequence, n_features, window_size=10):\n",
    "    \n",
    "    start = 0\n",
    "    data = list()\n",
    "    while start < len(sequence):\n",
    "        end = start + window_size\n",
    "        \n",
    "        if end > len(sequence):\n",
    "            break\n",
    "            \n",
    "        chunk = sequence[start:end]\n",
    "        # Using 50 % overlap for shifting window\n",
    "        start = start + int(window_size/2)\n",
    "        data.append(chunk)\n",
    "\t\n",
    "    \n",
    "    X = np.array(data)\n",
    "    print(\"shape: \", X.shape)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def create_subsequences_for_prediction(sequence, n_features, window_size=10):\n",
    "    \n",
    "    start = 0\n",
    "    data = list()\n",
    "    while start < len(sequence):\n",
    "        end = start + window_size\n",
    "        \n",
    "        if end > len(sequence):\n",
    "            break\n",
    "            \n",
    "        chunk = sequence.iloc[start:end:,]\n",
    "        #print(chunk.shape)\n",
    "        # Using 50 % overlap for shifting window\n",
    "        start = start + window_size\n",
    "        data.append(chunk)\n",
    "\t\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka7ArlbjaI8n"
   },
   "source": [
    "## Reconstruction Error and Threshold Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPciwNC_ZGTM"
   },
   "source": [
    "### Defining feature-wise reconstruction error computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6oPfklejJPL"
   },
   "outputs": [],
   "source": [
    "def calculate_reconstruction_errors(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Z6P_Dqxj2Dx"
   },
   "outputs": [],
   "source": [
    "def calculate_reconstruction_errors_list_of_outputs(y_true, y_pred):\n",
    "    reconstruction_errors = []\n",
    "    for i, data in enumerate(y_true):\n",
    "        \n",
    "        predicted_value = y_pred[i]\n",
    "        reconstruction_error = mean_squared_error(data, predicted_value)\n",
    "        reconstruction_errors = np.append(reconstruction_errors, reconstruction_error)\n",
    "\n",
    "    return reconstruction_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1X0k-ECIZACn"
   },
   "source": [
    "### Fitting a Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UE3bvCJZAMl"
   },
   "outputs": [],
   "source": [
    "def fit_gaussian(error_vectors):\n",
    "      \n",
    "    # featurewise mean\n",
    "    mean = np.mean(error_vectors, axis = 0 , dtype=np.float64)\n",
    "    print(\"mean: \", mean)\n",
    "    #computing standard deviation\n",
    "    std = np.std(error_vectors, axis = 0 , dtype=np.float64)\n",
    "    print(\"std: \", std)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKNiHB0ZZN3-"
   },
   "source": [
    "### Threshold computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRY93QdqZV3J"
   },
   "outputs": [],
   "source": [
    "# sigma specifies the amount of deviation tolerated\n",
    "def calculate_threshold(error_vectors, sigma=3):\n",
    "    \n",
    "    mean, std = fit_gaussian(get_error_vectors(error_vectors))\n",
    "    reconstruction_error_threshold_low = mean - (sigma * std)\n",
    "    reconstruction_error_threshold_high = mean + (sigma * std)\n",
    "    \n",
    "    #print(reconstruction_error_threshold_low)\n",
    "    \n",
    "    return reconstruction_error_threshold_low, reconstruction_error_threshold_high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CTFDBFwZXrx"
   },
   "source": [
    "### Check for anomalous states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4Uz632umiqM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method flags large deviations in the reconstruction errors observed in the reconstructed sequence.\n",
    "If they are beyond the tolerance level identified by the defined threshold, an anomaly is flagged.\n",
    "'''\n",
    "def is_anomaly(error_vector, reconstruction_error_threshold_low, reconstruction_error_threshold_high):\n",
    "    \n",
    "    result = 0\n",
    "    is_an_anomaly = False\n",
    "    node_list = []\n",
    "    index = 99999\n",
    "    \n",
    "    #print(\"threshold low: \", reconstruction_error_threshold_low)\n",
    "    #print(\"threshold high: \", reconstruction_error_threshold_high)\n",
    "    \n",
    "    #print(\"Error: \", error_vector)\n",
    "    \n",
    "    num_samples= 1\n",
    "    for seq in range(num_samples):\n",
    "        error_array = np.array(error_vector[seq])\n",
    "        \n",
    "        #print(\"error: \", error_vector)\n",
    "        \n",
    "        #print(\"length: \", len(reconstruction_error_threshold_low))\n",
    "        \n",
    "        for i in range(len(reconstruction_error_threshold_low)):\n",
    "        \n",
    "            score = [1 if(error > reconstruction_error_threshold_high[i]) else 0 for error in error_array[:,i]]\n",
    "            #print(\"score: \", score)\n",
    "            c = Counter(score)\n",
    "            if c[1] > 0:\n",
    "                #print(\"Alert in Node: \", i)\n",
    "                node_list.append(i)\n",
    "                if score.index(1) < index:\n",
    "                    index = score.index(1)\n",
    "                is_an_anomaly |= True\n",
    "    \n",
    "\n",
    "        if is_an_anomaly:\n",
    "            result = 1\n",
    "    \n",
    "    return result, node_list, index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjqS_ybD5qx9"
   },
   "source": [
    "## Prediction and Reconstruction error Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyU-yiVp5zl0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method flattens the errors observed in the window of \n",
    "input sequences into a list of error vectors.\n",
    "'''\n",
    "def get_error_vectors(errors):\n",
    "    vectors = []\n",
    "    for i in range(len(errors)):\n",
    "        for j in range(errors[i].shape[0]):\n",
    "            vectors.append(errors[i][j])\n",
    "    error_vectors = np.array(vectors)\n",
    "    \n",
    "    return error_vectors\n",
    "\n",
    "\n",
    "'''\n",
    "This method provides a prediction on the reconstruction of input sequence. Additionally,\n",
    "all reconstruction errors and computed and returned along with the predictions.\n",
    "It essentially does a model.predict\n",
    "'''\n",
    "def predict(X, model, scaler):\n",
    "    \n",
    "    #print(\"Model loaded is: \", model.name)\n",
    "    reconstructions = model.predict(X, verbose=0)\n",
    "    predictions = scaler.inverse_transform(reconstructions)\n",
    "    X = scaler.inverse_transform(X)\n",
    "    \n",
    "    #Compute errors in predictions\n",
    "    errors = np.zeros((predictions.shape[0], predictions.shape[1], predictions.shape[2]))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        errors[i] = np.abs(X[i] - predictions[i])\n",
    "        #print(\"errors: \", errors[i])\n",
    "    \n",
    "    return errors, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QACS8CKgUou"
   },
   "source": [
    "## MAIN TRAIN AND TEST PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgTaN3Rm_QI5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method represents the training phase of the algorithm.\n",
    "Offline training is done on the collected metric data.\n",
    "'''\n",
    "def train_main(data_path):\n",
    "    \n",
    "    input_dim = 3 # set to the number of nodes\n",
    "    timesteps = 60\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    df_val = pd.read_csv(val_data_path)\n",
    " \n",
    "        \n",
    "    # Dictionary to store the models corresponding to differemt metrics. The importance of the metric can be configured \n",
    "    # by specifying the weight. Ensure that they all sum up to 1. Any additional metric can be added here and the same will\n",
    "    # be picked up for analysis by our system.\n",
    "    params = {} \n",
    "    params[\"cpu\"] = {\"filename\" : \"data_cpu_final.csv\", \"weight\" : \"0.5\"}\n",
    "    params[\"mem\"] = {\"filename\" : \"data_mem_final.csv\", \"weight\" : \"0.5\"}\n",
    "    \n",
    "    \n",
    "    for metric_type in params.keys():\n",
    "        \n",
    "        if(metric_type == \"cpu\"):\n",
    "            data = df.iloc[:,1:4]\n",
    "            val_data = df_val.iloc[:,1:4]\n",
    "        else:\n",
    "            data = df.iloc[:, 4:7]\n",
    "            val_data = df_val.iloc[:,4:7]\n",
    "        \n",
    "        data.dropna(inplace = True)\n",
    "        params = train_pipeline(data, val_data, metric_type, params, data_path, timesteps, input_dim)\n",
    "    \n",
    "    # dump the parameters, models and weights into the filesystem\n",
    "    with open(data_path + \"paramsDict\", 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    " \n",
    "\n",
    "'''\n",
    "This method encompasses the entire train pipeline\n",
    "1. Standardize the data\n",
    "2. Create subsequences and reshape them to be fed into the LSTM Autoencoder model\n",
    "3. Create a model or load an existing one\n",
    "4. Fit the model on the training data\n",
    "5. Make predictions on the validation set to compute threshold or tolerance on reconstruction errors\n",
    "6. Compute the thresholds on reconstruction error\n",
    "7. Save the trained model and update the parameter dictionary with the saved model path\n",
    "'''\n",
    "def train_pipeline(data, val_data, metric_type, params, data_path, timesteps, input_dim):\n",
    "    \n",
    "    print(\"Inside train-pipeline: \", metric_type)\n",
    "    \n",
    "    # standardize\n",
    "    data_standardized, scaler = standardize(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    X = create_subsequence(data_standardized, input_dim, window_size = timesteps)\n",
    "    \n",
    "    # create model \n",
    "    model = get_model(timesteps, input_dim)\n",
    "    model.name = metric_type\n",
    "    \n",
    "    # Fit model\n",
    "    history_callback = model.fit(X, X, batch_size=32, epochs=20, validation_split=0.10)\n",
    "   \n",
    "    # Plot the validation and train losses\n",
    "#         fig = plt.figure()\n",
    "#         ax = plt.subplot(111)\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.xticks(range(1, 21, 1))\n",
    "#         y = range(1, 21)\n",
    "#         c1 = plt.plot(y, np.squeeze(history_callback.history['loss']), color=\"teal\", label=\"Training\")\n",
    "#         c2 = plt.plot(y, np.squeeze(history_callback.history['val_loss']), color=\"orange\", label=\"Validation\")\n",
    "#         ax.legend()\n",
    "#         plt.title(\"Train vs Validation Loss\")\n",
    "#         plt.savefig(data_path + \"trainval.png\")\n",
    "#         plt.show()\n",
    "    \n",
    "    # Plot the validation and train accuracies\n",
    "#         fig = plt.figure()\n",
    "#         ax = plt.subplot(111)\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#         plt.xticks(range(1, 21, 1))\n",
    "#         y = range(1, 21)\n",
    "#         c1 = plt.plot(y, np.squeeze(history_callback.history['acc']), color=\"teal\", label=\"Training\")\n",
    "#         c2 = plt.plot(y, np.squeeze(history_callback.history['val_acc']), color=\"orange\", label=\"Validation\")\n",
    "#         ax.legend()\n",
    "#         plt.title(\"Train vs Validation Accuracy\")\n",
    "#         plt.savefig(data_path + \"trainvalAcc.png\")\n",
    "#         plt.show()\n",
    "\n",
    "    \n",
    "    # Predict on train\n",
    "    print(\"Metric type is :\", metric_type)\n",
    "    errors, reconstructions = predict(val_data, model, scaler)\n",
    "    \n",
    "    # Compute Threshold\n",
    "    sigma = 3\n",
    "    threshold_low, threshold_high = calculate_threshold(errors, sigma)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(data_path + metric_type + \"_model.hdf5\")\n",
    "    \n",
    "    # Add scaler, thresholds, model save path to the dictionary\n",
    "    d = params.get(metric_type)\n",
    "    d[\"scaler\"] = scaler\n",
    "    d[\"model_save_path\"] = data_path + metric_type\n",
    "    d[\"t_low\"] = threshold_low\n",
    "    d[\"t_high\"] = threshold_high\n",
    "    params[metric_type] = d\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "'''\n",
    "This method has two major responsibilities:\n",
    "1. Make predictions on the new incoming sequences\n",
    "2. Compare the reconstruction errors alogn with the defined thresholds and flag if its an anomalous sequence.\n",
    "'''\n",
    "\n",
    "def test_pipeline(data, metric_type, params, input_dim, timesteps, metric_model):\n",
    "    \n",
    "    print(\"Making Predictions\")\n",
    "    #print(\"Metric type is :\", metric_type)\n",
    "    #print(\"Model loaded is: \", metric_model.name)\n",
    "    \n",
    "    # Predict on test\n",
    "    errors, predictions = predict(data, metric_model, params.get(metric_type)[\"scaler\"])\n",
    "       \n",
    "    # Check if anomaly\n",
    "    return is_anomaly(errors, params.get(metric_type)[\"t_low\"], params.get(metric_type)[\"t_high\"])\n",
    "  \n",
    "\n",
    "'''\n",
    "This method encompasses the entire test pipeline\n",
    "1. Identify the CPU and memory SLO violation thresholds\n",
    "2. Fetch real-time data from time-series database of Prometheus\n",
    "3. Load the CPU and Memory LSTM Autoencoder models\n",
    "4. Create subsequences and reshape them to be fed into the LSTM Autoencoder model\n",
    "5. Standardize the data\n",
    "6. Make predictions\n",
    "7. Keep track of anomaly count. If 40 consecutive anomalous patterns observed, update the model to capture the context drift\n",
    "8. Compute the new thresholds on reconstruction error\n",
    "9. Save the trained model and update the parameter dictionary with the saved model path\n",
    "'''\n",
    "def test_main(train_data_path, input_dim=3, timesteps=60):\n",
    "    \n",
    "    \n",
    "    # We load the train data to get the 99th percentile value for cpu and 95 percentile value for observed memory usage.\n",
    "    # We set these values as thresholds for tagging SLO violations.\n",
    "    \n",
    "    dataset = pd.read_csv(train_data_path, index_col=0)\n",
    "    metric = 'collectd_cpu_percent'\n",
    "    cpu_slo_violation = [np.percentile(dataset[metric+'_node1'], 99), np.percentile(dataset[metric+'_node2'], 99), np.percentile(dataset[metric+'_node3'], 99)]\n",
    "    metric = \"memory\"\n",
    "    memory_slo_violation = [np.percentile(dataset[metric+'_node1'], 95), np.percentile(dataset[metric+'_node2'], 95), np.percentile(dataset[metric+'_node3'], 99)]\n",
    "    metric_list = [\"collectd_cpu_percent\", \"collectd_memory{memory='used'}\", \"collectd_memory{memory='cached'}\", \n",
    "              \"collectd_memory{memory='buffered'}\"]\n",
    "    \n",
    "    print(cpu_slo_violation, memory_slo_violation)\n",
    "    \n",
    "    \n",
    "    # Fetch real-time data from time-series database of Prometheus\n",
    "    data = get_data_x_minutes_back_as_dataframe(metric_list, 2, datetime.now())\n",
    "    \n",
    "    # drop NA if any\n",
    "    data.dropna(inplace = True)\n",
    "    \n",
    "    #load params dict\n",
    "    params = pickle.load(open(data_path + \"paramsDict\", \"rb\" ))\n",
    "    \n",
    "    # Loading the CPU and Memory LSTM Autoencoder models\n",
    "    model_path = params.get(\"cpu\")[\"model_save_path\"]\n",
    "    cpu_model = load_model(model_path + \"_model.hdf5\")\n",
    "    \n",
    "    model_path = params.get(\"mem\")[\"model_save_path\"]\n",
    "    mem_model = load_model(model_path + \"_model.hdf5\")\n",
    "    \n",
    "    num_features_in_data = 6\n",
    "    X = create_subsequences_for_prediction(data, num_features_in_data, window_size = timesteps)\n",
    "    \n",
    "    anomaly_count = 0\n",
    "    slo_count = 0\n",
    "    start_timestamp = 0\n",
    "    \n",
    "    anomaly_score = 0\n",
    "    metric_dict = {}\n",
    "    metric_dict_slo = {}\n",
    "    print(\"Starting predictions\")\n",
    "    idx = 0\n",
    "    for data1 in X:\n",
    "        anomaly_score = 0\n",
    "        \n",
    "        for metric_type in params.keys():\n",
    "            \n",
    "            if metric_type == \"cpu\":\n",
    "                metric_data = data1.iloc[:,0:3]\n",
    "                violation = cpu_slo_violation\n",
    "                metric_model = cpu_model\n",
    "                \n",
    "            else:\n",
    "                metric_data = data1.iloc[:,3:6]\n",
    "                violation = memory_slo_violation\n",
    "                metric_model = mem_model\n",
    "            \n",
    "            #print(data1)\n",
    "            #print(metric_type)\n",
    "            #print(metric_data)\n",
    "            #print(metric_data)\n",
    "            data_standardized = params.get(metric_type)[\"scaler\"].transform(metric_data)\n",
    "            data_standardized = data_standardized.reshape((1, timesteps, input_dim))\n",
    "            \n",
    "            start = time.time()\n",
    "            score, node_list, index = test_pipeline(data_standardized, metric_type, params, input_dim, timesteps, metric_model) \n",
    "            end = time.time()\n",
    "            print(\"Time taken : \", end - start)\n",
    "            \n",
    "            \n",
    "            if(len(node_list) > 0):\n",
    "                \n",
    "                d = dict()\n",
    "                d[\"Node_list\"] = node_list\n",
    "                d[\"time\"] = data.index[idx + index]\n",
    "                metric_dict[metric_type] = d\n",
    "            \n",
    "            anomaly_score += score * float(params.get(metric_type)[\"weight\"])\n",
    "\n",
    "        #print(\"anomaly_score: \", anomaly_score)\n",
    "        \n",
    "        # If anomoly score is greater than or equal to 0.5, we flag it as an anomaly\n",
    "        is_anomaly_score = anomaly_score >= 0.5\n",
    "\n",
    "        if is_anomaly_score:\n",
    "            print(\"ANOMALY PREDICTION MADE. ALERT!\")\n",
    "            print(\"WATCH OUT FOR:\", metric_dict)\n",
    "            print()\n",
    "            send_to_slack(\"ANOMALY PREDICTION MADE. CHECK \"+ str(metric_dict))\n",
    "            anomaly_count += 1\n",
    "            \n",
    "        \n",
    "        # Checking for SLO violations\n",
    "        metric_dict_slo = {}\n",
    "        for metric_type in params.keys():\n",
    "            \n",
    "            if metric_type == \"cpu\":\n",
    "                metric_data = data1.iloc[:,0:3]\n",
    "                violation = cpu_slo_violation\n",
    "            else:\n",
    "                metric_data = data1.iloc[:,3:6]\n",
    "                violation = memory_slo_violation\n",
    "                \n",
    "            index_of_slo_violation = 9999   \n",
    "            for t in range(len(metric_data)):\n",
    "                \n",
    "                if (metric_data.iloc[t, 0] > violation[0]) or (metric_data.iloc[t, 1] > violation[1]) or (metric_data.iloc[t, 2] > violation[2]):\n",
    "                    index_of_slo_violation = t\n",
    "                    \n",
    "            if(index_of_slo_violation < 9999):\n",
    "                node_list_cpu = list()\n",
    "                    \n",
    "                if (metric_data.iloc[index_of_slo_violation, 0] >= violation[0]):\n",
    "                    node_list_cpu.append(0)\n",
    "                    \n",
    "                if (metric_data.iloc[index_of_slo_violation, 1] >= violation[1]):\n",
    "                    node_list_cpu.append(1)\n",
    "                    \n",
    "                if (metric_data.iloc[index_of_slo_violation, 2] >= violation[2]):\n",
    "                    node_list_cpu.append(2)\n",
    "                    \n",
    "                    \n",
    "                d = dict()\n",
    "                d[\"Node_list\"] = node_list_cpu\n",
    "                d[\"time\"] = data.index[idx + index_of_slo_violation]\n",
    "                metric_dict_slo[metric_type] = d\n",
    "         \n",
    "        # Here, we identify if node list is empty or not. Only if its not empty, we shall\n",
    "        # add it to the list\n",
    "        add = 0\n",
    "        for key in metric_dict_slo.keys():\n",
    "            if(len(metric_dict_slo.get(key)[\"Node_list\"]) > 0):\n",
    "                add = 1\n",
    "        \n",
    "        \n",
    "        slo_count = slo_count + add\n",
    "        # If we observe 3 consecutive SLO violations, a violation is notified.\n",
    "        if slo_count == 3:\n",
    "            for key in metric_dict_slo.keys():\n",
    "                if(len(metric_dict_slo.get(key)[\"Node_list\"]) > 0):\n",
    "                    print(\"SLO VIOLATION:\", metric_dict_slo)\n",
    "            slo_count = 0\n",
    "                \n",
    "                \n",
    "        # increment index to proceed to the next window   \n",
    "        idx = idx + timesteps\n",
    "        \n",
    "        \n",
    "        \n",
    "       # This part of the code handles context drift. If the LSTM Autoencoder model reports an\n",
    "       # anomaly for 40 consecutive times, we identify this as possible shift in context and hence update the model and\n",
    "       # threshold values.\n",
    "\n",
    "       #if anomaly_count > 40:\n",
    "            #update_model_and_thresholds(params, start_timestamp)\n",
    "\n",
    "       #if anomaly_count == 1:\n",
    "            #start_timestamp = data[0]\n",
    "\n",
    "        #else:\n",
    "            #anomaly_count = 0\n",
    "\n",
    "            \n",
    "            \n",
    "## Online model update code. \n",
    "## Note: this code is experimental. It is currently being tested\n",
    "def update_model_and_thresholds(params, start_timestamp, data_path=\"\"):\n",
    "    # get data from utility api, range query from start timestamp to now\n",
    "    \n",
    "    data = get_data_range(start_timestamp, datetime.now(), \"cpu\")\n",
    "    data.dropna(inplace = True)\n",
    "    params = train_pipeline(data, data, \"cpu\", params, data_path, 60, 3)\n",
    "    \n",
    "    data = get_data_range(start_timestamp, datetime.now(), \"mem\")\n",
    "    data.dropna(inplace = True)\n",
    "    params = train_pipeline(data, data, \"mem\", params, data_path, 60, 3)\n",
    "    \n",
    "    # dump the parameters, models and weights into the filesystem\n",
    "    with open(data_path + \"paramsDict\", 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7v3VOB7c-9z"
   },
   "source": [
    "## Plotting predictions of the LSTM Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh-6Jiom66q3"
   },
   "outputs": [],
   "source": [
    "def plot_prediction(x, y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Plots the predictions.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x: Input sequence of shape (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    y_true: True output sequence of shape (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    y_pred: Predicted output sequence (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    ax = plt.subplot(111)\n",
    "    output_dim = x.shape[-1]\n",
    "    #plt.xticks(range(0, len(past), 1))\n",
    "    for j in range(output_dim):\n",
    "        past = x[:, j] \n",
    "        true = y_true[:, j]\n",
    "        pred = y_pred[:, j]\n",
    "\n",
    "        label1 = \"Seen (past) values\" if j==0 else \"_nolegend_\"\n",
    "        label2 = \"True future values\" if j==0 else \"_nolegend_\"\n",
    "        label3 = \"Predictions\" if j==0 else \"_nolegend_\"\n",
    "\n",
    "        plt.plot(range(len(past)), past, \"o--b\",\n",
    "                 label=label1, color=\"blue\")\n",
    "        plt.plot(range(len(past),\n",
    "                 len(true)+len(past)), true, \"x--b\", label=label2, color=\"teal\", markersize=12)\n",
    "        plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\",\n",
    "                 label=label3, color=\"orange\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(\"Predictions v.s. true values\")\n",
    "    #ax.set_ylim(ymin=0, ymax = 20, auto = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-sZIexWydGrk"
   },
   "source": [
    "## Plot the model training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rszisai8uTR_"
   },
   "outputs": [],
   "source": [
    "def plot_scores(data1, data2, y, xlabel, ylabel, learning_rate, filename, label1, label2):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    c1 = plt.plot(y, np.squeeze(data1), color=\"teal\", label=label1)\n",
    "    c2 = plt.plot(y, np.squeeze(data2), color=\"orange\", label=label2)\n",
    "    ax.legend()\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    #plt.title(\"Optimizer = Adam(lr=0.0001)\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the functions for test or train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if len(sys.argv) < 3:\n",
    "        print(\"Insufficient arguments\")\n",
    "        sys.exit()\n",
    "    if sys.argv[1] == \"train\":\n",
    "        data_path = sys.argv[2]\n",
    "        train_main(data_path)\n",
    "    if sys.argv[1] == \"test\":\n",
    "        data_path = sys.argv[2]\n",
    "        test_main(data_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Autoencoder.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
